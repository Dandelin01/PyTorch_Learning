{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dandelin01/PyTorch_Learning/blob/main/main/_generated_ipynb_notebooks/plot_scripted_tensor_transforms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOoRx3Bc8scb"
      },
      "source": [
        "\n",
        "# Torchscript support\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [Colab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_scripted_tensor_transforms.ipynb)\n",
        "    or `go to the end <sphx_glr_download_auto_examples_others_plot_scripted_tensor_transforms.py>` to download the full example code.</p></div>\n",
        "\n",
        "This example illustrates [torchscript](https://pytorch.org/docs/stable/jit.html) support of the torchvision\n",
        "`transforms <transforms>` on Tensor images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFHbF2vr8scc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision.transforms as v1\n",
        "from torchvision.io import decode_image\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# If you're trying to run that on Colab, you can download the assets and the\n",
        "# helpers from https://github.com/pytorch/vision/tree/main/gallery/\n",
        "import sys\n",
        "sys.path += [\"../transforms\"]\n",
        "from helpers import plot\n",
        "ASSETS_PATH = Path('../assets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvrFPg-58scd"
      },
      "source": [
        "Most transforms support torchscript. For composing transforms, we use\n",
        ":class:`torch.nn.Sequential` instead of\n",
        ":class:`~torchvision.transforms.v2.Compose`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGCxr3fU8scd"
      },
      "outputs": [],
      "source": [
        "dog1 = decode_image(str(ASSETS_PATH / 'dog1.jpg'))\n",
        "dog2 = decode_image(str(ASSETS_PATH / 'dog2.jpg'))\n",
        "\n",
        "transforms = torch.nn.Sequential(\n",
        "    v1.RandomCrop(224),\n",
        "    v1.RandomHorizontalFlip(p=0.3),\n",
        ")\n",
        "\n",
        "scripted_transforms = torch.jit.script(transforms)\n",
        "\n",
        "plot([dog1, scripted_transforms(dog1), dog2, scripted_transforms(dog2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXnarE0A8sce"
      },
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Above we have used transforms from the ``torchvision.transforms``\n",
        "    namespace, i.e. the \"v1\" transforms. The v2 transforms from the\n",
        "    ``torchvision.transforms.v2`` namespace are the `recommended\n",
        "    <v1_or_v2>` way to use transforms in your code.\n",
        "\n",
        "    The v2 transforms also support torchscript, but if you call\n",
        "    ``torch.jit.script()`` on a v2 **class** transform, you'll actually end up\n",
        "    with its (scripted) v1 equivalent.  This may lead to slightly different\n",
        "    results between the scripted and eager executions due to implementation\n",
        "    differences between v1 and v2.\n",
        "\n",
        "    If you really need torchscript support for the v2 transforms, **we\n",
        "    recommend scripting the functionals** from the\n",
        "    ``torchvision.transforms.v2.functional`` namespace to avoid surprises.</p></div>\n",
        "\n",
        "Below we now show how to combine image transformations and a model forward\n",
        "pass, while using ``torch.jit.script`` to obtain a single scripted module.\n",
        "\n",
        "Let's define a ``Predictor`` module that transforms the input tensor and then\n",
        "applies an ImageNet model on it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3mBvJoF8sce"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        weights = ResNet18_Weights.DEFAULT\n",
        "        self.resnet18 = resnet18(weights=weights, progress=False).eval()\n",
        "        self.transforms = weights.transforms(antialias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            x = self.transforms(x)\n",
        "            y_pred = self.resnet18(x)\n",
        "            return y_pred.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR5o_BUp8sce"
      },
      "source": [
        "Now, let's define scripted and non-scripted instances of ``Predictor`` and\n",
        "apply it on multiple tensor images of the same size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8WZeJQS8sce"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = Predictor().to(device)\n",
        "scripted_predictor = torch.jit.script(predictor).to(device)\n",
        "\n",
        "batch = torch.stack([dog1, dog2]).to(device)\n",
        "\n",
        "res = predictor(batch)\n",
        "res_scripted = scripted_predictor(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxCfO-sy8scf"
      },
      "source": [
        "We can verify that the prediction of the scripted and non-scripted models are\n",
        "the same:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_8XvA-B8scf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(Path('../assets') / 'imagenet_class_index.json') as labels_file:\n",
        "    labels = json.load(labels_file)\n",
        "\n",
        "for i, (pred, pred_scripted) in enumerate(zip(res, res_scripted)):\n",
        "    assert pred == pred_scripted\n",
        "    print(f\"Prediction for Dog {i + 1}: {labels[str(pred.item())]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTFCUgCd8scf"
      },
      "source": [
        "Since the model is scripted, it can be easily dumped on disk and re-used\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlzcajLI8scf"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile() as f:\n",
        "    scripted_predictor.save(f.name)\n",
        "\n",
        "    dumped_scripted_predictor = torch.jit.load(f.name)\n",
        "    res_scripted_dumped = dumped_scripted_predictor(batch)\n",
        "assert (res_scripted_dumped == res_scripted).all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}